{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManassiLab/SR_Consistency/blob/main/Copia_di_Face_Id_Analysis_Split_half_analysis_Within_consistency_3BINS_newVersion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "Y5ZnhPFsfOoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy\n",
        "import math\n",
        "import statistics\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "#from lmfit import Model\n",
        "from numpy import exp, loadtxt, sqrt, std, mean\n",
        "from scipy import stats\n",
        "from scipy.stats import sem\n",
        "from scipy.stats import pearsonr\n",
        "#from lmfit.models import GaussianModel\n",
        "from scipy.stats import linregress\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "pd.set_option('display.max_columns', 30)\n",
        "pd.set_option('display.max_rows', None)\n",
        "%matplotlib inline\n",
        "#%pip install lmfit\n",
        "#sys.path.insert(-1, \"c:\\python\\lib\\site-packages\\lmfit\")\n",
        "\n",
        "print(\"import packages DONE\")"
      ],
      "metadata": {
        "id": "WdD6-z-NfRn0",
        "outputId": "c21d3e6e-4ae7-4e2f-f7f6-09db5a08f164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import packages DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "5xK-c83LfTGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ActualStim = 'stimulusID'\n",
        "#StartResp = 'StartBar'\n",
        "Response = 'morphID'\n",
        "trialsLoop = 'trialNumber' #trialnum per block trialsLoop.thisRepN\n",
        "blocksLoop = 'blockNumber' #blocknum blocksLoop.thisRepN\n",
        "#GaborTime = 'GaborTime'\n",
        "#MaskTime = 'MaskTime'\n",
        "#ISItime = 'ISI' #blocknum blocksLoop.thisRepN\n",
        "#TimeResp = 'RespTime'\n",
        "#ITI = 'ITI'\n",
        "Location = 'stimLocationDeg'\n",
        "#Gender = '*Geschlecht (M/W/anders)'\n",
        "#age = '*Geburtsjahr '\n",
        "#PersonalCode = 'Bitte hier eingeben: (1) die ersten 2 Buchstaben des Vornamens Ihrer Mutter, (2) der Tag (des Monats) an dem Sie geboren wurden, (3) die letzten 2 Buchstaben Ihres Vornamens, (4) der Tag (des Monats) an dem Ihre Mutter geboren wurde (Bsp: LY16KE26)'\n",
        "blockType_ = 'blockType'\n",
        "error_corr = 'errorCorrected'\n",
        "SDcutoffYN = 0\n",
        "FilterYN = 0\n",
        "halfway = 74 # circular space correction (74 morphs, 90 orientation)\n",
        "CurrentPath = os. getcwd()\n",
        "\n",
        "print(\"Parameters DONE\")\n",
        "print(\"Current Path is        \" + CurrentPath)\n",
        "pathSTART = CurrentPath + \"/SR_Consistency-main/Data/\"\n",
        "print(\"Looking into           \" + pathSTART)\n"
      ],
      "metadata": {
        "id": "RLTxXZM4fViE",
        "outputId": "fdfbe3d3-dad0-4069-d6c3-33cd046fe449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters DONE\n",
            "Current Path is        /content\n",
            "Looking into           /content/SR_Consistency-main/Data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get all files**"
      ],
      "metadata": {
        "id": "PK8s03jkfenH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def GetFromSingleCsv(path):\n",
        "\n",
        "### LOOK FOR FILES IN FOLDER\n",
        "    files = os.listdir(path) # list of ALL files\n",
        "    print(\"Files found in folder:\", files)\n",
        "    files_csv = list(filter(lambda x: x[-4:] == '.csv' , files)) # list of all CSV files\n",
        "\n",
        "    FileNameList = []\n",
        "    for file in files_csv:\n",
        "        FileName = file[-7:-4]\n",
        "        FileNameList.append(FileName) #to give random name to file saved NOT USED\n",
        "\n",
        "    ##### if result exists, don't run function again\n",
        "    ResultPath = path + 'results/'\n",
        "\n",
        "    ##### show CSV files to analyze\n",
        "    print('CSV FILES FOUND AND READ:',files_csv)\n",
        "\n",
        "### DATA ASSEMBLING\n",
        "    # define a void list to store data\n",
        "    data_list = pd.DataFrame() #dataframe\n",
        "\n",
        "    #read useful columns, save in all_data\n",
        "    for file in  files_csv:\n",
        "        tmp = (pd.read_csv(path + file)[[ActualStim, Response, error_corr, trialsLoop, blocksLoop,Location,blockType_]]) #add variables you want here!\n",
        "        all_data = pd.concat([data_list, tmp],ignore_index=True) #if all together: ignore_index=True\n",
        "\n",
        "    # computes trial number per block, block number and total trial number\n",
        "    file_csv = len(files_csv) #only for online studies, total of files analyzed\n",
        "    trials = (np.nanmax(all_data[trialsLoop])) #+1 because it starts at 0\n",
        "    blocks = (np.nanmax(all_data[blocksLoop])) #+1 because it starts at 0\n",
        "\n",
        "    PerFile = (trials*blocks) # Trials in each csv file\n",
        "    BlocksTotal = blocks*file_csv # Blocks in all csv file\n",
        "    TotalTrial = trials * blocks * file_csv # SuperTotal Trials\n",
        "\n",
        "    print(file_csv,'file_csv(s):\\n', int(trials),'trials/blocks.', int(blocks),'blocks - each file_csv.\\n', int(PerFile) ,'trials - each file_csv.\\n', int(BlocksTotal),'total blocks TOTAL.', int(TotalTrial),'total trials TOTAL.')\n",
        "\n",
        "    # delete invalid rows and useless columns\n",
        "    all_data.dropna(axis = 0, how = 'any', inplace = True) # exclude NaN (often between blocks)\n",
        "    all_data = all_data[all_data['blockType']=='Experiment']\n",
        "    #MM qui potresti fare il concatenate in 3D, cosi hai una 2d matrix per ogni subj\n",
        "    return all_data\n"
      ],
      "metadata": {
        "id": "GdiT6se0fgAz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get Data in**"
      ],
      "metadata": {
        "id": "rYv51NPVfjCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def DataIn(all_data, path):\n",
        "\n",
        "    xdata = all_data['stimulusID']\n",
        "    CurrStim = np.array(xdata)\n",
        "    Respdata = all_data['morphID']\n",
        "    ydata = all_data[error_corr] #np.array(Respdata)-np.array(xdata) # ERROR\n",
        "    \n",
        "#FILTER FOR OUTLIERS IN ERROR (ydata)\n",
        "    if SDcutoffYN:\n",
        "        ErrorCutoff = 2 * std(ydata) # otherwise ErrorCutoff at the beginning\n",
        "    if FilterYN == 1:\n",
        "        xdata = xdata[abs(ydata) < ErrorCutoff]\n",
        "        CurrStim = CurrStim[abs(ydata) < ErrorCutoff]\n",
        "        Blocks_ = Blocks_[abs(ydata) < ErrorCutoff]\n",
        "        ydata = ydata[abs(ydata) < ErrorCutoff]\n",
        "        std_ydata=np.std(ydata)\n",
        "        std_ydata=round(std_ydata,2)\n",
        "        print(\"Resp Err NO FILTER =\",std_ydataNOFILTER,\"Response Error After Outlier Removal =\",std_ydata,\", cutoff threshold:\", int(ErrorCutoff), \",\", \"trials removed\", int(ydataAfterConditions - len(ydata)))\n",
        "    else:\n",
        "        std_ydata=np.std(ydata)\n",
        "        std_ydata=round(std_ydata,2)\n",
        "        print(\"Response Error No Filter =\",std_ydata)\n",
        "    \n",
        "    #compensates for biases\n",
        "    #ydata = ydata-np.mean(ydata)---> DA FARE??  \n",
        "    \n",
        "#DEFINING GRAPH VARIABLES\n",
        "    ydata_neg = ydata #error, with negative values   #rel_error e #abs_error\n",
        "    ydata = abs(ydata) #absolute error\n",
        "    df = pd.DataFrame({\"ActualZ\": xdata, \"Error\": ydata, \"Error_neg\": ydata_neg})\n",
        "\n",
        "#GROUPS INTO BINS    \n",
        "    slices2 = np.arange(1,147,3)\n",
        "    slices2 = slices2.tolist()\n",
        "    \n",
        "    TotBins= []\n",
        "    for x in slices2:\n",
        "        bins_= df[(df[\"ActualZ\"]>=x) & (df[\"ActualZ\"]<x+3)]\n",
        "        bins_2 = list(bins_[\"Error_neg\"])\n",
        "        TotBins.append(bins_2)                  \n",
        "    TotBins_allsubjects.append(TotBins) \n",
        "    \n",
        "    return TotBins_allsubjects\n"
      ],
      "metadata": {
        "id": "DxFIQx9CflJN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Script**"
      ],
      "metadata": {
        "id": "OS6tFm0kfqXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################Runs Function#############################################\n",
        "\n",
        "All_ErrorOut = []\n",
        "TotAge = []\n",
        "TotAmplitude = []\n",
        "TotAmplitude2 = []\n",
        "Totmean = []\n",
        "Totmean_abs = []\n",
        "SEM = []\n",
        "Totmean1 = []\n",
        "TotBins_allsubjects = []\n",
        "\n",
        "\n",
        "Subj = sorted(os.listdir(pathSTART))\n",
        "print(\"Folders found: \", Subj)\n",
        "TotalSubj = np.arange(0, len(Subj))\n",
        "\n",
        "for i in TotalSubj:\n",
        "    all_data = []\n",
        "    TotBins_allsubjects = []\n",
        "    path = pathSTART + Subj[i] + \"/\"\n",
        "    print(\"SUBJECT FROM:\", path)\n",
        "    all_data = GetFromSingleCsv(path) #goes into each folder, reads csv and gets all_data\n",
        "    TotBins_allsubjects = DataIn(all_data,path)\n",
        "    print()\n",
        "    print()\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "Dh4IyP0QfsvM",
        "outputId": "a636c9dd-e66b-492e-8a74-6ed43fdca34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-02f4f57edc92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mSubj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathSTART\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Folders found: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mTotalSubj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSubj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/SR_Consistency-main/Data/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Within-Consistency Analysis**"
      ],
      "metadata": {
        "id": "PBKg7DIugwye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Data Within-Subject Consistency**"
      ],
      "metadata": {
        "id": "vqYYFCJEg2fy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copia di Face_Id_Analysis_Split-half analysis_Within-consistency_3BINS_newVersion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}