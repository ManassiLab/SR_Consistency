{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fiammetta/Desktop/tirocinio/SuperRecognizers/TryConsistencyAnalysis/Data/\n",
      "['S8']\n",
      "[0]\n",
      "SUBJECT FROM:\n",
      "/Users/Fiammetta/Desktop/tirocinio/SuperRecognizers/TryConsistencyAnalysis/Data/S8/\n",
      "['Bootstrap_Within-subject_consistency.png', 'Bootstrap_Within-subject_consistency_3bins.png', 'S8_AdjustEyeTrack_2019_Dec_04_1101 copy.csv']\n",
      "['S8_AdjustEyeTrack_2019_Dec_04_1101 copy.csv']\n",
      "FILES FOUND AND READ: ['S8_AdjustEyeTrack_2019_Dec_04_1101 copy.csv']\n",
      "1 file_csv(s):\n",
      " 85 trials/blocks. 3 blocks - each file_csv.\n",
      " 255 trials - each file_csv.\n",
      " 3 total blocks TOTAL. 255 total trials TOTAL.\n",
      "[[[-64.0, 18.0, 3.0], [-4.0, -4.0, -10.0, 0.0, 3.0, 0.0, 7.0], [-1.0, -8.0, -18.0, 6.0, -8.0], [-7.0, -5.0, -8.0, -1.0], [-8.0, -6.0, -4.0, 9.0, -7.0], [-4.0, -6.0, 0.0, -3.0, -4.0], [31.0, 4.0, -9.0, 4.0, -68.0], [-1.0, 4.0, 1.0, 1.0, -2.0, 23.0, -8.0], [-1.0, -1.0, 3.0, 9.0, -6.0], [51.0, -2.0, -1.0, -2.0, -2.0, -4.0], [-1.0, -1.0, 1.0, -2.0, -2.0, -10.0], [-2.0, -3.0, -4.0, -8.0, 1.0, -7.0], [-5.0, 5.0, 2.0, -36.0, 1.0], [-31.0, -2.0, -3.0, -25.0, 5.0], [-9.0, 0.0, -1.0], [43.0, -9.0, 4.0, -3.0, -17.0], [-12.0, -15.0, -14.0, -2.0, -5.0], [-13.0, -4.0, -18.0, 7.0, 7.0], [8.0, -27.0, 4.0, -19.0, 2.0], [-12.0, -20.0, 14.0, 1.0, -18.0, -15.0, -19.0], [-1.0, -8.0, -3.0, -6.0], [-11.0, -8.0, -15.0, 22.0, -10.0], [-26.0, -5.0, -7.0, -11.0, -9.0], [-4.0, -5.0, -2.0, -5.0, -5.0, 2.0], [-3.0, -4.0, 15.0, 0.0, 2.0], [2.0, 4.0, 3.0, -5.0, -13.0], [-17.0, -2.0, -2.0, 8.0], [3.0, 8.0, -15.0, -18.0, -6.0, -16.0], [-2.0, -9.0, -5.0, -38.0, -18.0], [5.0, -30.0, -25.0, -23.0, 0.0, -1.0, -3.0], [-16.0, -6.0, -5.0, 11.0, -9.0, 7.0], [14.0, -11.0, -9.0, 4.0, -40.0], [1.0, -1.0, 0.0, 2.0], [11.0, -3.0, 8.0, 15.0], [11.0, 8.0, 7.0, 13.0, 10.0], [-12.0, 5.0, 2.0, 13.0, 11.0, -4.0], [11.0, 12.0, 11.0, 13.0, 20.0], [22.0, 10.0, 12.0, 0.0, 34.0, 22.0], [-54.0, 4.0, 16.0, 19.0], [24.0, 22.0, -12.0, -2.0], [23.0, 36.0, 21.0, -37.0, -15.0, -28.0], [5.0, 19.0, -17.0, 33.0, 15.0], [-15.0, -17.0, 20.0, 17.0, 34.0, -17.0, 37.0], [8.0, 46.0, -9.0, -10.0, 5.0], [55.0, -4.0, -8.0], [13.0, 55.0, 10.0, -9.0, -8.0, -16.0, -40.0], [-15.0, -2.0, 3.0, -8.0, -6.0], [-5.0, 0.0, 2.0, -4.0], [-5.0, -5.0, -3.0, 2.0, -4.0, -1.0, -4.0, -1.0]]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "#\n",
    "\n",
    "# In[1]:\n",
    "import sys\n",
    "sys.path.insert(-1, \"c:\\python\\lib\\site-packages\\lmfit\")\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from lmfit import Model\n",
    "from numpy import exp, loadtxt, sqrt, std, mean\n",
    "from scipy import stats\n",
    "from scipy.stats import sem\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import math\n",
    "import statistics\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "FolderPathUsers = 'Fiammetta/Desktop/tirocinio' #OR, 'mauromanassi/Dropbox/MANASSILAB/zzz_FIAMMETTA' \n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def RunAnalysis(path):\n",
    "\n",
    "### Parameters\n",
    "\n",
    "    #### User's Input\n",
    "    halfway = 74 # circular space correction (74 morphs, 90 orientation)\n",
    "    step = 20 # for running mean, how wide to run avg.\n",
    "    nBack = 1 #how many trials to go back/forward if+ 1Back if- 1Forward\n",
    "    Forward = 0 #0 not forward, 1 is forward\n",
    "    Timecutoff = 10 # if exceeds this RT, then filtered\n",
    "    ErrorCutoff = 40 # for error (bar - gabor) filter\n",
    "    polynum = 15 # polynomial correction, highest exponent\n",
    "\n",
    "\n",
    "    #### Control console\n",
    "    TimelimitYN = 1 # remove over-RT?\n",
    "    SDcutoffYN = 0 # cutoff as 2 s.d.\n",
    "    FilterYN = 1 # Filter data for Error?\n",
    "    polyYN = 0 # do polynomial correction?\n",
    "    DoGYN = 1 # plot DoG curve?\n",
    "    bootstrapYN = 0 # do bootstrap sampling?\n",
    "    permYN = 0 # do permutation sampling?\n",
    "\n",
    "\n",
    "\n",
    "### Looks For Files In Folder\n",
    "\n",
    "    ##### read all csv files\n",
    "    files = os.listdir(path) # list of ALL files\n",
    "    print(files)\n",
    "    files_csv = list(filter(lambda x: x[-4:] == '.csv' , files)) # list of all CSV files\n",
    "    print(files_csv)\n",
    "    FileNameList = []\n",
    "    for file in files_csv:\n",
    "        FileName = file[-7:-4]\n",
    "        FileNameList.append(FileName) #to give random name to file saved NOT USED\n",
    "\n",
    "    ##### if result exists, don't run function again\n",
    "    ResultPath = path + 'results/'\n",
    "\n",
    "    ##### show CSV files to analyze\n",
    "    print('FILES FOUND AND READ:',files_csv)\n",
    "\n",
    "### Data Assembling\n",
    "\n",
    "    #MAIN VARIABLES\n",
    "    ActualStim = 'stimulusID'\n",
    "    #StartResp = 'StartBar'\n",
    "    Response = 'morphID'\n",
    "    trialsLoop = 'trialNumber' #trialnum per block trialsLoop.thisRepN\n",
    "    blocksLoop = 'blockNumber' #blocknum blocksLoop.thisRepN\n",
    "    #GaborTime = 'GaborTime'\n",
    "    #MaskTime = 'MaskTime'\n",
    "    #ISItime = 'ISI' #blocknum blocksLoop.thisRepN\n",
    "    #TimeResp = 'RespTime'\n",
    "    #ITI = 'ITI'\n",
    "    Location = 'stimLocationDeg'\n",
    "    #Gender = '*Geschlecht (M/W/anders)'\n",
    "    #age = '*Geburtsjahr '\n",
    "    #PersonalCode = 'Bitte hier eingeben: (1) die ersten 2 Buchstaben des Vornamens Ihrer Mutter, (2) der Tag (des Monats) an dem Sie geboren wurden, (3) die letzten 2 Buchstaben Ihres Vornamens, (4) der Tag (des Monats) an dem Ihre Mutter geboren wurde (Bsp: LY16KE26)'\n",
    "    blockType_ = 'blockType'\n",
    "    error_corr = 'errorCorrected'\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # define a void list to store data\n",
    "    data_list = pd.DataFrame() #dataframe\n",
    "\n",
    "    # transformation\n",
    "    #num_filter = re.compile(r'\\d+')\n",
    "\n",
    "    #read useful columns, save in all_data\n",
    "    for file in  files_csv:\n",
    "        tmp = (pd.read_csv(path + file)[[ActualStim, Response, error_corr, trialsLoop, blocksLoop,Location,blockType_]])\n",
    "        all_data = pd.concat([data_list, tmp],ignore_index=True) #if all together: ignore_index=True\n",
    "        \n",
    "    #print(all_data)\n",
    "\n",
    "    \n",
    "    # computes trial number per block, block number and total trial number\n",
    "    file_csv = len(files_csv) #only for online studies, total of files analyzed\n",
    "    trials = (np.nanmax(all_data[trialsLoop])) #+1 because it starts at 0\n",
    "    blocks = (np.nanmax(all_data[blocksLoop])) #+1 because it starts at 0\n",
    "\n",
    "    PerFile = (trials*blocks) # Trials in each csv file\n",
    "    BlocksTotal = blocks*file_csv # Blocks in all csv file\n",
    "    TotalTrial = trials * blocks * file_csv # SuperTotal Trials\n",
    "\n",
    "    print(file_csv,'file_csv(s):\\n', int(trials),'trials/blocks.', int(blocks),'blocks - each file_csv.\\n', int(PerFile) ,'trials - each file_csv.\\n', int(BlocksTotal),'total blocks TOTAL.', int(TotalTrial),'total trials TOTAL.')\n",
    "\n",
    "    # delete invalid rows and useless columns\n",
    "    all_data.dropna(axis = 0, how = 'any', inplace = True) # exclude NaN (often between blocks)\n",
    "    all_data = all_data[all_data['blockType']=='Experiment']\n",
    "    #print(all_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "### Data Assembling\n",
    "\n",
    "    #plot_dataTOTAL = all_data\n",
    "\n",
    "    xdata = all_data['stimulusID']\n",
    "    CurrStim = np.array(xdata)\n",
    "    Respdata = all_data['morphID']\n",
    "    ydata = all_data[error_corr] #np.array(Respdata)-np.array(xdata) # ERROR\n",
    "    \n",
    "            \n",
    "            \n",
    "    # Filter for outliers in error(ydata)\n",
    "   # if SDcutoffYN:\n",
    "   #     ErrorCutoff = 2 * std(ydata) # otherwise ErrorCutoff at the beginning\n",
    "   # if FilterYN == 1:\n",
    "   #     xdata = xdata[abs(ydata) < ErrorCutoff]\n",
    "   #     CurrStim = CurrStim[abs(ydata) < ErrorCutoff]\n",
    "   #     #Blocks_ = Blocks_[abs(ydata) < ErrorCutoff]\n",
    "   #     ydata = ydata[abs(ydata) < ErrorCutoff]\n",
    "   #     std_ydata=np.std(ydata)\n",
    "   #     std_ydata=round(std_ydata,2)\n",
    "   #     #print(\"Resp Err NO FILTER =\",std_ydataNOFILTER,\"Response Error After Outlier Removal =\",std_ydata,\", cutoff threshold:\", int(ErrorCutoff), \",\", \"trials removed\", int(ydataAfterConditions - len(ydata)))\n",
    "   # else:\n",
    "   #     std_ydata=np.std(ydata)\n",
    "   #     std_ydata=round(std_ydata,2)\n",
    "   #     #print(\"Response Error No Filter =\",std_ydata)\n",
    "   # \n",
    "    \n",
    "    #compensates for biases\n",
    "    #ydata = ydata-np.mean(ydata)---> DA FARE??  \n",
    "    \n",
    "    \n",
    "    #DEFINING GRAPH VARIABLES\n",
    "    ydata_neg = ydata #error, with negative values   #rel_error e #abs_error\n",
    "    ydata = abs(ydata) #absolute error\n",
    "\n",
    "        \n",
    "    #### CREATE A DATAFRAME\n",
    "    df = pd.DataFrame({\"ActualZ\": xdata, \"Error\": ydata, \"Error_neg\": ydata_neg})\n",
    "    pd.set_option(\"max_rows\", None)\n",
    "    df = df.sort_values(by=[\"ActualZ\"])\n",
    "    #print(df)\n",
    "\n",
    "    #Bin the errors in 49 bins (3 current orientations in each bin)\n",
    "    #slices = np.linspace(1, 145, (49), endpoint=False).astype(int)\n",
    "    #print(slices)\n",
    "    #print(len(slices))\n",
    "\n",
    "    slices2 = []\n",
    "    for x in range(-2,145,3):\n",
    "        x2 = x+3\n",
    "        #print(x2)\n",
    "        slices2.append(x2)\n",
    "    #print(slices2)\n",
    "    #print(len(slices2))\n",
    "    \n",
    "    TotBins= []\n",
    "    for x in slices2:\n",
    "        #print(x)\n",
    "        bins_= df[(df[\"ActualZ\"]>=x) & (df[\"ActualZ\"]<x+3)]\n",
    "        #print(bins_)\n",
    "        bins_2 = list(bins_[\"Error_neg\"])\n",
    "        #print(bins_2)\n",
    "        TotBins.append(bins_2)                  \n",
    "    \n",
    "    \n",
    "    TotBins_allsubjects.append(TotBins)\n",
    "        \n",
    "    print(TotBins_allsubjects)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# ### Run Function ###\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#############################################Runs Function#############################################\n",
    "pathSTART = \"/Users/\" + FolderPathUsers + \"/SuperRecognizers/TryConsistencyAnalysis/Data/\"\n",
    "\n",
    "\n",
    "All_ErrorOut = []\n",
    "TotAge = []\n",
    "TotAmplitude = []\n",
    "TotAmplitude2 = []\n",
    "Totmean = []\n",
    "Totmean_abs = []\n",
    "SEM = []\n",
    "Totmean1 = []\n",
    "\n",
    "TotBins_allsubjects = []\n",
    "\n",
    "\n",
    "\n",
    "print(pathSTART)\n",
    "Subj = sorted(os.listdir(pathSTART))\n",
    "print(Subj)\n",
    "TotalSubj = np.arange(0, len(Subj))\n",
    "print(TotalSubj)\n",
    "for i in TotalSubj:\n",
    "    path = pathSTART + Subj[i] + \"/\"\n",
    "    print(\"SUBJECT FROM:\")\n",
    "    print(path)\n",
    "    RunAnalysis(path)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   4   7  10  13  16  19  22  25  28  31  34  37  40  43  46  49  52\n",
      "  55  58  61  64  67  70  73  76  79  82  85  88  91  94  97 100 103 106\n",
      " 109 112 115 118 121 124 127 130 133 136 139 142 145] 49\n"
     ]
    }
   ],
   "source": [
    "#print(TotBins_allsubjects[9])\n",
    "\n",
    "a= np.arange(1,147,3)\n",
    "print(a, len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04465513647774614\n",
      "[-0.04465513647774614]\n",
      "-0.04465513647774614\n"
     ]
    }
   ],
   "source": [
    "### Within-subject consistency--> Bootstrap method to estimate split-half correlations \n",
    "from lmfit.models import GaussianModel\n",
    "from scipy.stats import linregress\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "def split_list(a_list):\n",
    "    half = len(a_list)//2\n",
    "    return a_list[:half], a_list[half:]\n",
    "\n",
    "    \n",
    "def flatten(seq): \n",
    "    l = []\n",
    "    for elt in seq:\n",
    "        t = type(elt)\n",
    "        if t is tuple or t is list:\n",
    "            for elt2 in flatten(elt):\n",
    "                l.append(elt2)\n",
    "        else:\n",
    "            l.append(elt)\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#On each iteration, for each observer and each binned morph, randomly split the responses into two halves \n",
    "\n",
    "All_iterations_Average_r_Pearson = []\n",
    "iterations = 0\n",
    "\n",
    "\n",
    "#CorrSubj = []\n",
    "#for b in range (1000):\n",
    "#    bin_n1 = []\n",
    "#    bin_n2 = []\n",
    "#    Tot_MEANbin_1 = []\n",
    "#    Tot_MEANbin_2 = []\n",
    "#    NAN_ = 0\n",
    "#    iterations += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "ALL_CorrSubj = []\n",
    "for x in range(len(Subj)):\n",
    "    CorrSubj = []\n",
    "    for b in range (1000):\n",
    "        mean_bin1_singleSubject = []\n",
    "        mean_bin2_singleSubject = []\n",
    "        for bin in range(49):\n",
    "            #print(bin)\n",
    "            #randomly shuffle the values in the bin\n",
    "            actual_bin = (TotBins_allsubjects[x])[bin]\n",
    "            random.shuffle(actual_bin)\n",
    "            \n",
    "            #randomly split the responses into two halves \n",
    "            a, b = split_list(actual_bin)\n",
    "            \n",
    "            if len(a) > len(b):\n",
    "                del a[-1]\n",
    "            if len(b) > len(a):\n",
    "                del b[-1]\n",
    "                            \n",
    "            \n",
    "           #alert if a bin is = []        \n",
    "            if a == [] and b == []:\n",
    "                a = 0\n",
    "                b = 0\n",
    "                #print(\"NANN\")\n",
    "                #print(bin)\n",
    "           #if a bin = [], repeat the value of the other bin (this is to avoid NaN)\n",
    "            if a == []:\n",
    "                a = b\n",
    "                #NAN_ = NAN_ + 1\n",
    "                #print(\"NAN\")\n",
    "                \n",
    "            if b == []:\n",
    "                b = a\n",
    "                #NAN_ = NAN_ + 1\n",
    "                #print(\"NAN\")\n",
    "    \n",
    "            #calculate the mean response errors for each half\n",
    "            mean_bin1 = mean(a)\n",
    "            mean_bin2 = mean(b)\n",
    "            \n",
    "            mean_bin1_singleSubject.append(mean_bin1)\n",
    "            mean_bin2_singleSubject.append(mean_bin2)\n",
    "            \n",
    "        #The two halves are correlated--> Pearson’s r value  \n",
    "        corr, _ = pearsonr(mean_bin1_singleSubject, mean_bin2_singleSubject)\n",
    "        #print(corr)\n",
    "        corr = np.arctanh(corr)\n",
    "        #print(corr)\n",
    "        CorrSubj.append(corr)\n",
    "\n",
    "    print(mean(CorrSubj))\n",
    "    ALL_CorrSubj.append(mean(CorrSubj))\n",
    "    \n",
    "print(ALL_CorrSubj)\n",
    "print(mean(ALL_CorrSubj))\n",
    "#z_values = np.arctanh(CorrSubj)\n",
    "#z_values_mean= mean(z_values)\n",
    "#print(z_values_mean)\n",
    "\n",
    "        \n",
    "        \n",
    "#\n",
    "#    #The two halves are correlated--> Pearson’s r value  \n",
    "#    Tot_halves_correlations = []\n",
    "#    Tot_z_values = []\n",
    "#    for x in range(len(Tot_MEANbin_1)):\n",
    "#        Pvalue_halves = linregress(Tot_MEANbin_1[x], Tot_MEANbin_2[x])\n",
    "#        Tot_halves_correlations.append(Pvalue_halves[2])\n",
    "#        \n",
    "#        #Pearson’s r value is transformed into a Fisher z value \n",
    "#        #print(Pvalue_halves[2])\n",
    "#        z_values = np.arctanh(Pvalue_halves[2])\n",
    "#        #print(z_values)\n",
    "#        Tot_z_values.append(z_values)\n",
    "#        \n",
    "#        \n",
    "#    #print(Tot_halves_correlations)\n",
    "#    #print(Tot_z_values)\n",
    "#    \n",
    "#    #average z values of all the subjects\n",
    "#    Average_Tot_z_values = mean(Tot_z_values)\n",
    "#    #print(Average_Tot_z_values)\n",
    "#    \n",
    "#    #trasform the average z value in r value \n",
    "#    Average_r_Pearson = np.tanh(Average_Tot_z_values)\n",
    "#    All_iterations_Average_r_Pearson.append(Average_r_Pearson)\n",
    "#    \n",
    "#    print(\"NUMERO DI NAN: \", NAN_)\n",
    "#    print(\"ITERATIONS: \", iterations)\n",
    "    \n",
    "#print(All_iterations_Average_r_Pearson)\n",
    "    \n",
    "\n",
    "    \n",
    "##HISTOGRAM BOOTSTRAP DATASET\n",
    "#fig5 = plt.figure(figsize=(10,8))\n",
    "#\n",
    "#plt.hist(All_iterations_Average_r_Pearson, bins=100)\n",
    "#plt.xlabel('Bootstrap r coefficient of correlation\\n',  fontsize = 15)\n",
    "#plt.axis([0, 0.50, 0, 40])\n",
    "#plt.yticks(range(0, 45, 5))\n",
    "#plt.xticks([0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "#\n",
    "#meanAmp = np.mean(All_iterations_Average_r_Pearson)\n",
    "#conf_interval1 = np.percentile(All_iterations_Average_r_Pearson,[2.5])\n",
    "#conf_interval2 = np.percentile(All_iterations_Average_r_Pearson,[97.5])\n",
    "#print(meanAmp)\n",
    "#print(conf_interval1)\n",
    "#print(conf_interval2)\n",
    "#\n",
    "##p-value\n",
    "#TotBoot_means2 = np.array(All_iterations_Average_r_Pearson)\n",
    "#s0 = 0\n",
    "#pval = 1-(sum(TotBoot_means2 >= s0)/len(TotBoot_means2))\n",
    "#print(pval)\n",
    "#\n",
    "#title = \"IDENTITY SD EXPERIMENT\\n\" + \"Mean: \" + str(np.round(meanAmp, decimals = 3)) + \"  CI low: \" + str(np.round(conf_interval1, decimals = 3)) + \"  CI high: \" + str(np.round(conf_interval2, decimals = 3))+ \"\\np-value: \" + str(round(pval, 3))\n",
    "#\n",
    "#plt.title(title, fontsize = 20)\n",
    "#\n",
    "#plt.vlines(conf_interval1, 0, 40, colors='k', linestyles='solid')\n",
    "#plt.vlines(conf_interval2, 0, 40, colors='k', linestyles='solid')\n",
    "#plt.vlines(meanAmp, 0, 40, colors='k', linestyles='--')\n",
    "#\n",
    "#\n",
    "#\n",
    "#plt.show()\n",
    "#\n",
    "#FigName = \"/Users/\" + FolderPathUsers + \"/SuperRecognizers/TryConsistencyAnalysis/all_graphs/Bootstrap_Within-subjects_3bin.png\"\n",
    "##FigName = path + ('Bootstrap_' + BackOrForward + str(nBack) + '_Data.png')\n",
    "#\n",
    "#fig5.tight_layout()\n",
    "#fig5.savefig(FigName, dpi=600)\n",
    "#\n",
    "#plt.clf()    \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(All_iterations_Average_r_Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meanAmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-433b41cf079d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfig1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeanAmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'meanAmp' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT BAR GRAPH WITHIN-SUBJECT CONSISTENCY\n",
    "\n",
    "list1 = [1]\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.bar(list1, meanAmp, width=0.8, align='center')\n",
    "\n",
    "plt.axis([0, 2, -0.1, 0.8]) \n",
    "#plt.xticks([\"Control group\"])\n",
    "#plt.yticks(np.arange(0, 60, 5))\n",
    "\n",
    "plt.errorbar(list1, meanAmp, yerr=(conf_interval1-meanAmp, meanAmp-conf_interval2), fmt=' ', ecolor='black', capsize=5)\n",
    "\n",
    "title = \"WITHIN-SUBJECT CONSISTENCY\\n\" + \"Mean: \" + str(np.round(meanAmp, decimals = 2))  \n",
    "plt.title(title, fontsize = 20)\n",
    "plt.xlabel(\"\\nRadiologist group\", fontsize = 20)\n",
    "plt.ylabel(\"Pearson's R\", fontsize = 20)\n",
    "plt.tick_params(labelsize = 15 ,bottom=False, labelbottom=False)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "FigName = \"/Users/\" + FolderPathUsers + \"/SuperRecognizers/TryConsistencyAnalysis/all_graphs/Bootstrap_Within-subjects_BarGraph_3bin.png\"\n",
    "\n",
    "\n",
    "fig1.tight_layout()\n",
    "fig1.savefig(FigName, dpi=600)\n",
    "\n",
    "plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
